<!doctype html><html lang=en><head><meta charset=utf-8><meta http-equiv=X-UA-Compatible content="IE=edge,chrome=1"><title>The Perceptron - Beyond Lambda</title>
<meta name=renderer content="webkit"><meta name=viewport content="width=device-width,initial-scale=1,maximum-scale=1"><meta http-equiv=Cache-Control content="no-transform"><meta http-equiv=Cache-Control content="no-siteapp"><meta name=theme-color content="#f8f5ec"><meta name=msapplication-navbutton-color content="#f8f5ec"><meta name=apple-mobile-web-app-capable content="yes"><meta name=apple-mobile-web-app-status-bar-style content="#f8f5ec"><meta name=author content><meta name=description content="The perceptron is a learning algorithm. A rather simple one yet surprising, it can acheive very good results as we will explore in this post.
"><meta name=generator content="Hugo 0.129.0 with theme even"><link rel=canonical href=https://mostafaeissa.github.io/post/the-perceptron/><link rel=apple-touch-icon sizes=180x180 href=/apple-touch-icon.png><link rel=icon type=image/png sizes=32x32 href=/favicon-32x32.png><link rel=icon type=image/png sizes=16x16 href=/favicon-16x16.png><link rel=manifest href=/manifest.json><link rel=mask-icon href=/safari-pinned-tab.svg color=#5bbad5><link href=/sass/main.min.f92fd13721ddf72129410fd8250e73152cc6f2438082b6c0208dc24ee7c13fc4.css rel=stylesheet><link href=/lib/fancybox/jquery.fancybox-3.1.20.min.css rel=stylesheet><meta property="og:url" content="https://mostafaeissa.github.io/post/the-perceptron/"><meta property="og:site_name" content="Beyond Lambda"><meta property="og:title" content="The Perceptron"><meta property="og:description" content="The perceptron is a learning algorithm. A rather simple one yet surprising, it can acheive very good results as we will explore in this post."><meta property="og:locale" content="en_us"><meta property="og:type" content="article"><meta property="article:section" content="post"><meta property="article:published_time" content="2020-02-26T00:00:00+00:00"><meta property="article:modified_time" content="2020-02-26T00:00:00+00:00"><meta itemprop=name content="The Perceptron"><meta itemprop=description content="The perceptron is a learning algorithm. A rather simple one yet surprising, it can acheive very good results as we will explore in this post."><meta itemprop=datePublished content="2020-02-26T00:00:00+00:00"><meta itemprop=dateModified content="2020-02-26T00:00:00+00:00"><meta itemprop=wordCount content="1946"><meta itemprop=keywords content="Machine Learning"><meta name=twitter:card content="summary"><meta name=twitter:title content="The Perceptron"><meta name=twitter:description content="The perceptron is a learning algorithm. A rather simple one yet surprising, it can acheive very good results as we will explore in this post."><!--[if lte IE 9]><script src=https://cdnjs.cloudflare.com/ajax/libs/classlist/1.1.20170427/classList.min.js></script><![endif]--><!--[if lt IE 9]><script src=https://cdn.jsdelivr.net/npm/html5shiv@3.7.3/dist/html5shiv.min.js></script><script src=https://cdn.jsdelivr.net/npm/respond.js@1.4.2/dest/respond.min.js></script><![endif]--></head><body><div id=mobile-navbar class=mobile-navbar><div class=mobile-header-logo><a href=/ class=logo>Beyond Lambda</a></div><div class=mobile-navbar-icon><span></span>
<span></span>
<span></span></div></div><nav id=mobile-menu class="mobile-menu slideout-menu"><ul class=mobile-menu-list><a href=/><li class=mobile-menu-item>Home</li></a><a href=/post/><li class=mobile-menu-item>Archives</li></a><a href=/about/><li class=mobile-menu-item>About</li></a></ul></nav><div class=container id=mobile-panel><header id=header class=header><div class=logo-wrapper><a href=/ class=logo>Beyond Lambda</a></div><nav class=site-navbar><ul id=menu class=menu><li class=menu-item><a class=menu-item-link href=/>Home</a></li><li class=menu-item><a class=menu-item-link href=/post/>Archives</a></li><li class=menu-item><a class=menu-item-link href=/about/>About</a></li></ul></nav></header><main id=main class=main><div class=content-wrapper><div id=content class=content><article class=post><header class=post-header><h1 class=post-title>The Perceptron</h1><div class=post-meta><span class=post-time>2020-02-26</span><div class=post-category><a href=/categories/machine-learning/>Machine Learning</a></div><span class=more-meta>1946 words </span><span class=more-meta>10 mins read</span></div></header><div class=post-toc id=post-toc><h2 class=post-toc-title>Contents</h2><div class="post-toc-content always-active"><nav id=TableOfContents><ul><li><ul><li><ul><li><a href=#introduction>Introduction</a></li><li><a href=#the-perceptron-update-rule>The Perceptron Update Rule</a></li><li><a href=#worked-example>Worked Example</a></li><li><a href=#limitations-xor-example>Limitations: XOR Example</a></li><li><a href=#python-implementation>Python Implementation</a></li><li><a href=#conclusion>Conclusion</a></li></ul></li></ul></li></ul></nav></div></div><div class=post-content><p>The perceptron is a learning algorithm. A rather simple one yet surprising, it can acheive very good results as we will explore in this post.</p><h3 id=introduction>Introduction</h3><p>The perceptron is used for classification task where there are two categories and the objective is to learn how to differentiate between them. For example, whether an email is spam? Or whether a house will sell over a certain asking price?</p><p>Traditionally, for each object we want to classify a number of features are collected that best describe the object. In the house example mentioned earlier, the features might be the number of rooms, total area, number of bathrooms, etc. As a result we end up with a number of numerical features describing each object (instance) (Note: categorical features can be converted to numerical using one hot encoding) and our task can be described mathematically as given input instances $X$ of size $n$ where each instance is $d$-dimensional vector $a_1,a_2,a_3,….,a_d$ containing the feature values, design a function $h(x)$ that takes an input instance x and outputs $+1$ if $x$ belongs to the positive classs and $-1$ otherwise.</p><p>$$
\begin{equation}
h(x) =
\begin{cases}
+1 & if \enspace x \in \text{positive class} \newline
-1 & if \enspace x \in \text{negative class }
\end{cases}
\end{equation}
$$</p><p>In other words, each input is represented as a vector where each individual component is one of the features and the perceptron is trying to find a hyperplane (think line in 2D) to separate the instances of the $+1$ class from the instances of $-1$ class.</p><p>For simplicity, let us focus on the 2D plane and limit inputs to only two features $a_1$ and $a_2$. In this setting, each input corresponds to a point in 2D as seen in the next figure. The goal of the perceptron algorithm is to find a line such that all points on one side belong to the positive class and all points on the other side belong to the negative class.</p><div class=align-center><p><img src=/images/the-perceptron/the-separating-hyperplane.png alt="the separating hyperplane"></p></div><p>A plane can be described by a vector $w$ that is normal to the plane. Hence, when we say we want to find a separating plane we mean that we want to find the components of the vector $w$ that describe the plane.</p><p>Once the perceptron algorithm has found such hyperplane, we can classify instances based on the following rule. Note that the formula contains a bias term $b$ without this term, the hyperplane will always have to go through the origin</p><p>$$
\begin{equation}
h(x)=
\begin{cases}
+1 & if \enspace w.x+b > 0 \newline
-1 & if \enspace w.x+b &lt; 0
\end{cases}
\end{equation}$$</p><p>Here, $w.x$ is the dot product which is the sum component wise components of both vectors $\sum\limits_{i=1}^{d} w_i x_i$</p><h3 id=the-perceptron-update-rule>The Perceptron Update Rule</h3><p>The question now is how can we find such $w$ that classifies the points correctly. A simple idea is to initialize the $w$ vector randomly and go over the points one by one. If the point is correctly classified then we do not need to do anything however, if is misclassified we need to move the hyperplane (update $w$) so that the point will eventually become correctly classified. A good choice, as shown in the next figure, is a vector that is proportional to the misclassified data point but in the opposite direction i.e. $w_{new} =w_{old}+ηyx$. The algorithm keeps iterating until $w$ converges does not change anymore.</p><div class=align-center><p><img src=/images/the-perceptron/the-perceptron-update-rule.png alt="the perceptron update rule"></p></div><p>Each iteration of the algorithm consists of a full pass over all the training examples. The algorithm terminates when in a given iteration no point was misclassified.
A natural question at this point is how can we update the bias term $b$ as well. The trick is to augment our training data with all ones vector and treat the bias as another weight component (i.e. weight component of a feature whose value is alaways one).</p><p>$$
\begin{aligned}
h(x) &= w.x+b \newline
&= ∑<em>{i=1}^d w_i x_i+b \newline
&= ∑</em>{i=1}^d w_i x_i+b*1 \newline
&= ∑_{i=1}^{d+1} w_i x_i
\end{aligned}
$$</p><p>where $x_{d+1}$ always equals one and $w_{d+1}$ is the bias term</p><h3 id=worked-example>Worked Example</h3><p>To Understand the algorithm better, let’s go through a toy example. Suppose we have the following positive and negative examples in 3D</p><table><thead><tr><th style=text-align:center>Positive Examples</th><th style=text-align:center>Negative Examples</th></tr></thead><tbody><tr><td style=text-align:center>$[0 \enspace 0 \enspace 0]$</td><td style=text-align:center>$[0 \enspace 1 \enspace 1]$</td></tr><tr><td style=text-align:center>$[0 \enspace 0 \enspace 1]$</td><td style=text-align:center>$[0 \enspace 1 \enspace 0]$</td></tr><tr><td style=text-align:center>$[1 \enspace 0 \enspace 1]$</td><td style=text-align:center>$[1 \enspace 1 \enspace 0]$</td></tr><tr><td style=text-align:center>$[1 \enspace 0 \enspace 0]$</td><td style=text-align:center>$[1 \enspace 1 \enspace 1]$</td></tr></tbody></table><div class=align-center><p><img src=/images/the-perceptron/3d-example.png alt="3d example"></p></div><p><em><strong>Iteration 1</strong></em></p><table><thead><tr><th style=text-align:center>Weight<br>$[w_0 \enspace w_1 \enspace w_2 \enspace b]$</th><th style=text-align:center>Data<br>$[x_0 \enspace x_1 \enspace x_2 \enspace 1]$</th><th style=text-align:right><strong>${w.x}$</strong></th><th style=text-align:left>Label</th><th>Comment</th><th style=text-align:center>New weight</th></tr></thead><tbody><tr><td style=text-align:center>$[0 \enspace 0\enspace 0\enspace 0]$</td><td style=text-align:center>$[0 \enspace 0 \enspace 0 \enspace 1]$</td><td style=text-align:right>$0 \leq 0$</td><td style=text-align:left>$+$</td><td>Wrong. Add sample</td><td style=text-align:center>$[0 \enspace 0 \enspace 0 \enspace 1]$</td></tr><tr><td style=text-align:center>$[0 \enspace 0 \enspace 0 \enspace 1]$</td><td style=text-align:center>$[0 \enspace 0 \enspace 1 \enspace 1]$</td><td style=text-align:right>$1 > 0$</td><td style=text-align:left>$+$</td><td>OK</td><td style=text-align:center>No Change</td></tr><tr><td style=text-align:center>$[0 \enspace 0 \enspace 0 \enspace 1]$</td><td style=text-align:center>$[0 \enspace 1 \enspace 0 \enspace 1]$</td><td style=text-align:right>$1 > 0$</td><td style=text-align:left>$-$</td><td>Wrong. Subtract sample</td><td style=text-align:center>$[0 \enspace {-1} \enspace 0 \enspace 0]$</td></tr><tr><td style=text-align:center>$[0 \enspace {-1} \enspace 0 \enspace 0]$</td><td style=text-align:center>$[0 \enspace 1 \enspace 1 \enspace 1]$</td><td style=text-align:right>${-1} &lt; 0$</td><td style=text-align:left>$-$</td><td>OK</td><td style=text-align:center>No Change</td></tr><tr><td style=text-align:center>$[0 \enspace {-1} \enspace 0 \enspace 0]$</td><td style=text-align:center>$[1 \enspace 0 \enspace 0 \enspace 1]$</td><td style=text-align:right>$0 \leq 0$</td><td style=text-align:left>$+$</td><td>Wrong. Add sample</td><td style=text-align:center>$[1 \enspace {-1} \enspace 0 \enspace 1]$</td></tr><tr><td style=text-align:center>$[1 \enspace {-1} \enspace 0 \enspace 1]$</td><td style=text-align:center>$[1 \enspace 0 \enspace 1 \enspace 1]$</td><td style=text-align:right>$2 > 0$</td><td style=text-align:left>$+$</td><td>OK</td><td style=text-align:center>No Change</td></tr><tr><td style=text-align:center>$[1 \enspace {-1} \enspace 0 \enspace 1]$</td><td style=text-align:center>$[1 \enspace 1 \enspace 0 \enspace 1]$</td><td style=text-align:right>$1 > 0$</td><td style=text-align:left>$-$</td><td>Wrong. Subtract sample</td><td style=text-align:center>$[0 \enspace {-2} \enspace 0 \enspace 0]$</td></tr><tr><td style=text-align:center>$[0 \enspace {-2} \enspace 0 \enspace 0]$</td><td style=text-align:center>$[1 \enspace 1 \enspace 1 \enspace 1]$</td><td style=text-align:right>${-2} &lt; 0$</td><td style=text-align:left>$-$</td><td>OK</td><td style=text-align:center>No Change</td></tr></tbody></table><p><em><strong>Iteration 2</strong></em></p><table><thead><tr><th style=text-align:center>Weight<br>$[w_0 \enspace w_1 \enspace w_2 \enspace b]$</th><th style=text-align:center>Data<br>$[x_0 \enspace x_1 \enspace x_2 \enspace 1]$</th><th style=text-align:right><strong>${w.x}$</strong></th><th style=text-align:left>Label</th><th>Comment</th><th style=text-align:center>New weight</th></tr></thead><tbody><tr><td style=text-align:center>$[0 \enspace {-2}\enspace 0\enspace 0]$</td><td style=text-align:center>$[0 \enspace 0 \enspace 0 \enspace 1]$</td><td style=text-align:right>$0 \leq 0$</td><td style=text-align:left>$+$</td><td>Wrong. Add sample</td><td style=text-align:center>$[0 \enspace {-2} \enspace 0 \enspace 1]$</td></tr><tr><td style=text-align:center>$[0 \enspace {-2} \enspace 0 \enspace 1]$</td><td style=text-align:center>$[0 \enspace 0 \enspace 1 \enspace 1]$</td><td style=text-align:right>$1 > 0$</td><td style=text-align:left>$+$</td><td>OK</td><td style=text-align:center>No Change</td></tr><tr><td style=text-align:center>$[0 \enspace {-2} \enspace 0 \enspace 1]$</td><td style=text-align:center>$[0 \enspace 1 \enspace 0 \enspace 1]$</td><td style=text-align:right>${-1} &lt; 0$</td><td style=text-align:left>$-$</td><td>OK</td><td style=text-align:center>No Change</td></tr><tr><td style=text-align:center>$[0 \enspace {-2} \enspace 0 \enspace 1]$</td><td style=text-align:center>$[0 \enspace 1 \enspace 1 \enspace 1]$</td><td style=text-align:right>${-1} &lt; 0$</td><td style=text-align:left>$-$</td><td>OK</td><td style=text-align:center>No Change</td></tr><tr><td style=text-align:center>$[0 \enspace {-2} \enspace 0 \enspace 1]$</td><td style=text-align:center>$[1 \enspace 0 \enspace 0 \enspace 1]$</td><td style=text-align:right>$1 > 0$</td><td style=text-align:left>$+$</td><td>OK</td><td style=text-align:center>No Change</td></tr><tr><td style=text-align:center>$[0 \enspace {-2} \enspace 0 \enspace 1]$</td><td style=text-align:center>$[1 \enspace 0 \enspace 1 \enspace 1]$</td><td style=text-align:right>$1 > 0$</td><td style=text-align:left>$+$</td><td>OK</td><td style=text-align:center>No Change</td></tr><tr><td style=text-align:center>$[0 \enspace {-2} \enspace 0 \enspace 1]$</td><td style=text-align:center>$[1 \enspace 1 \enspace 0 \enspace 1]$</td><td style=text-align:right>${-1} &lt; 0$</td><td style=text-align:left>$-$</td><td>OK</td><td style=text-align:center>No Change</td></tr><tr><td style=text-align:center>$[0 \enspace {-2} \enspace 0 \enspace 1]$</td><td style=text-align:center>$[1 \enspace 1 \enspace 1 \enspace 1]$</td><td style=text-align:right>${-1} &lt; 0$</td><td style=text-align:left>$-$</td><td>OK</td><td style=text-align:center>No Change</td></tr></tbody></table><p><em><strong>Iteration 3</strong></em></p><table><thead><tr><th style=text-align:center>Weight<br>$[w_0 \enspace w_1 \enspace w_2 \enspace b]$</th><th style=text-align:center>Data<br>$[x_0 \enspace x_1 \enspace x_2 \enspace 1]$</th><th style=text-align:right><strong>${w.x}$</strong></th><th style=text-align:left>Label</th><th>Comment</th><th style=text-align:center>New weight</th></tr></thead><tbody><tr><td style=text-align:center>$[0 \enspace {-2}\enspace 0\enspace 1]$</td><td style=text-align:center>$[0 \enspace 0 \enspace 0 \enspace 1]$</td><td style=text-align:right>$1 > 0$</td><td style=text-align:left>$+$</td><td>OK</td><td style=text-align:center>No Change</td></tr><tr><td style=text-align:center>$[0 \enspace {-2} \enspace 0 \enspace 1]$</td><td style=text-align:center>$[0 \enspace 0 \enspace 1 \enspace 1]$</td><td style=text-align:right>$1 > 0$</td><td style=text-align:left>$+$</td><td>OK</td><td style=text-align:center>No Change</td></tr><tr><td style=text-align:center>$[0 \enspace {-2} \enspace 0 \enspace 1]$</td><td style=text-align:center>$[0 \enspace 1 \enspace 0 \enspace 1]$</td><td style=text-align:right>${-1} &lt; 0$</td><td style=text-align:left>$-$</td><td>OK</td><td style=text-align:center>No Change</td></tr><tr><td style=text-align:center>$[0 \enspace {-2} \enspace 0 \enspace 1]$</td><td style=text-align:center>$[0 \enspace 1 \enspace 1 \enspace 1]$</td><td style=text-align:right>${-1} &lt; 0$</td><td style=text-align:left>$-$</td><td>OK</td><td style=text-align:center>No Change</td></tr><tr><td style=text-align:center>$[0 \enspace {-2} \enspace 0 \enspace 1]$</td><td style=text-align:center>$[1 \enspace 0 \enspace 0 \enspace 1]$</td><td style=text-align:right>$1 > 0$</td><td style=text-align:left>$+$</td><td>OK</td><td style=text-align:center>No Change</td></tr><tr><td style=text-align:center>$[0 \enspace {-2} \enspace 0 \enspace 1]$</td><td style=text-align:center>$[1 \enspace 0 \enspace 1 \enspace 1]$</td><td style=text-align:right>$1 > 0$</td><td style=text-align:left>$+$</td><td>OK</td><td style=text-align:center>No Change</td></tr><tr><td style=text-align:center>$[0 \enspace {-2} \enspace 0 \enspace 1]$</td><td style=text-align:center>$[1 \enspace 1 \enspace 0 \enspace 1]$</td><td style=text-align:right>${-1} &lt; 0$</td><td style=text-align:left>$-$</td><td>OK</td><td style=text-align:center>No Change</td></tr><tr><td style=text-align:center>$[0 \enspace {-2} \enspace 0 \enspace 1]$</td><td style=text-align:center>$[1 \enspace 1 \enspace 1 \enspace 1]$</td><td style=text-align:right>${-1} &lt; 0$</td><td style=text-align:left>$-$</td><td>OK</td><td style=text-align:center>No Change</td></tr></tbody></table><p>So, the hyperplane found by the perceptron algorithm can be $[0 \enspace {-2} \enspace 0 \enspace 1]$</p><div class=align-center><p><img src=/images/the-perceptron/3d-example-solved.png alt="3d example solved"></p></div><h3 id=limitations-xor-example>Limitations: XOR Example</h3><p>However, there is one caveat for the Perceptron algorithm. The data needs to be linearly separable otherwise, the perceptron algorithm will fail to find a solution. To illustrate this point let’s consider the case of the XOR function. The XOR function outputs a 1 if only one of the inputs is one, otherwise it outputs zero.</p><table><thead><tr><th style=text-align:center>Input 1</th><th style=text-align:center>Input 2</th><th style=text-align:center>Output</th></tr></thead><tbody><tr><td style=text-align:center>0</td><td style=text-align:center>0</td><td style=text-align:center>0</td></tr><tr><td style=text-align:center>0</td><td style=text-align:center>1</td><td style=text-align:center>1</td></tr><tr><td style=text-align:center>1</td><td style=text-align:center>0</td><td style=text-align:center>1</td></tr><tr><td style=text-align:center>1</td><td style=text-align:center>1</td><td style=text-align:center>0</td></tr></tbody></table><p>We can visualize the XOR output in the 2D space as shown below. There is no line that can separate the positive from the negative points which means the data is not linearly separable.</p><div class=align-center><p><img src=/images/the-perceptron/XOR.png alt="XOR in 2D"></p></div><p>If we try to follow the same update rule specified earlier we will get stuck in an infinite loop.</p><p><em><strong>Iteration 1</strong></em></p><table><thead><tr><th style=text-align:center>Weight<br>$[w_0 \enspace w_1 \enspace b]$</th><th style=text-align:center>Data<br>$[x_0 \enspace x_1 \enspace 1]$</th><th style=text-align:right><strong>${w.x}$</strong></th><th style=text-align:left>Label</th><th>Comment</th><th style=text-align:center>New weight</th></tr></thead><tbody><tr><td style=text-align:center>$[0 \enspace 0\enspace 0]$</td><td style=text-align:center>$[0 \enspace 0 \enspace 1]$</td><td style=text-align:right>$0 \leq 0$</td><td style=text-align:left>$-$</td><td>OK</td><td style=text-align:center>No Change</td></tr><tr><td style=text-align:center>$[0 \enspace 0\enspace 0]$</td><td style=text-align:center>$[0 \enspace 1 \enspace 1]$</td><td style=text-align:right>$0 \leq 0$</td><td style=text-align:left>$+$</td><td>Wrong. Add sample</td><td style=text-align:center>$[0 \enspace 1\enspace 1]$</td></tr><tr><td style=text-align:center>$[0 \enspace 1\enspace 1]$</td><td style=text-align:center>$[1 \enspace 1 \enspace 1]$</td><td style=text-align:right>$2 > 0$</td><td style=text-align:left>$-$</td><td>Wrong. Subtract sample</td><td style=text-align:center>$[{-1} \enspace 0\enspace 0]$</td></tr><tr><td style=text-align:center>$[{-1} \enspace 0\enspace 0]$</td><td style=text-align:center>$[1 \enspace 0 \enspace 1]$</td><td style=text-align:right>${-1} &lt; 0$</td><td style=text-align:left>$+$</td><td>Wrong. Add sample</td><td style=text-align:center>$[0 \enspace 0\enspace 1]$</td></tr></tbody></table><p><em><strong>Iteration 2</strong></em></p><table><thead><tr><th style=text-align:center>Weight<br>$[w_0 \enspace w_1 \enspace b]$</th><th style=text-align:center>Data<br>$[x_0 \enspace x_1 \enspace 1]$</th><th style=text-align:right><strong>${w.x}$</strong></th><th style=text-align:left>Label</th><th>Comment</th><th style=text-align:center>New weight</th></tr></thead><tbody><tr><td style=text-align:center>$[0 \enspace 0\enspace 1]$</td><td style=text-align:center>$[0 \enspace 0 \enspace 1]$</td><td style=text-align:right>$1 > 0$</td><td style=text-align:left>$-$</td><td>Wrong. Subtract sample</td><td style=text-align:center>$[0 \enspace 0\enspace 0]$</td></tr><tr><td style=text-align:center>$[0 \enspace 0\enspace 0]$</td><td style=text-align:center>$[0 \enspace 1 \enspace 1]$</td><td style=text-align:right>$0 \leq 0$</td><td style=text-align:left>$+$</td><td>Wrong. Add sample</td><td style=text-align:center>$[0 \enspace 1\enspace 1]$</td></tr><tr><td style=text-align:center>$[0 \enspace 1\enspace 1]$</td><td style=text-align:center>$[1 \enspace 1 \enspace 1]$</td><td style=text-align:right>$2 > 0$</td><td style=text-align:left>$-$</td><td>Wrong. Subtract sample</td><td style=text-align:center>$[{-1} \enspace 0\enspace 0]$</td></tr><tr><td style=text-align:center>$[{-1} \enspace 0\enspace 0]$</td><td style=text-align:center>$[1 \enspace 0 \enspace 1]$</td><td style=text-align:right>${-1} &lt; 0$</td><td style=text-align:left>$+$</td><td>Wrong. Add sample</td><td style=text-align:center>$[0 \enspace 0\enspace 1]$</td></tr></tbody></table><p>And we are back to the same weight that we started the seconditeration with and we become stuck in the same pattern no matter how many passes we make over the data.</p><h3 id=python-implementation>Python Implementation</h3><p>We can easily implement the above update rule in a couple of lines of python.</p><div class=highlight><div class=chroma><table class=lntable><tr><td class=lntd><pre tabindex=0 class=chroma><code><span class=lnt> 1
</span><span class=lnt> 2
</span><span class=lnt> 3
</span><span class=lnt> 4
</span><span class=lnt> 5
</span><span class=lnt> 6
</span><span class=lnt> 7
</span><span class=lnt> 8
</span><span class=lnt> 9
</span><span class=lnt>10
</span><span class=lnt>11
</span><span class=lnt>12
</span><span class=lnt>13
</span><span class=lnt>14
</span><span class=lnt>15
</span><span class=lnt>16
</span><span class=lnt>17
</span><span class=lnt>18
</span><span class=lnt>19
</span><span class=lnt>20
</span><span class=lnt>21
</span><span class=lnt>22
</span><span class=lnt>23
</span><span class=lnt>24
</span><span class=lnt>25
</span><span class=lnt>26
</span><span class=lnt>27
</span></code></pre></td><td class=lntd><pre tabindex=0 class=chroma><code class=language-python data-lang=python><span class=line><span class=cl><span class=k>def</span> <span class=nf>perceptron</span><span class=p>(</span><span class=n>X</span><span class=p>,</span> <span class=n>y</span><span class=p>):</span>
</span></span><span class=line><span class=cl>    <span class=c1># append ones to input for bias</span>
</span></span><span class=line><span class=cl>    <span class=n>X</span> <span class=o>=</span> <span class=n>np</span><span class=o>.</span><span class=n>hstack</span><span class=p>((</span><span class=n>X</span><span class=p>,</span> <span class=n>np</span><span class=o>.</span><span class=n>ones</span><span class=p>((</span><span class=n>X</span><span class=o>.</span><span class=n>shape</span><span class=p>[</span><span class=mi>0</span><span class=p>],</span><span class=mi>1</span><span class=p>))))</span>
</span></span><span class=line><span class=cl>
</span></span><span class=line><span class=cl>    <span class=c1>#limit to number of iterations to do before giving up</span>
</span></span><span class=line><span class=cl>    <span class=n>max_iterations</span> <span class=o>=</span> <span class=mi>1000</span>
</span></span><span class=line><span class=cl>    <span class=n>curr_iter</span> <span class=o>=</span> <span class=mi>0</span>
</span></span><span class=line><span class=cl>
</span></span><span class=line><span class=cl>    <span class=n>n</span><span class=p>,</span><span class=n>d</span> <span class=o>=</span> <span class=n>X</span><span class=o>.</span><span class=n>shape</span>
</span></span><span class=line><span class=cl>    
</span></span><span class=line><span class=cl>    <span class=c1># initialize w to all zero vector</span>
</span></span><span class=line><span class=cl>    <span class=n>w</span> <span class=o>=</span> <span class=n>np</span><span class=o>.</span><span class=n>zeros</span><span class=p>(</span><span class=n>d</span><span class=p>)</span>
</span></span><span class=line><span class=cl>
</span></span><span class=line><span class=cl>    <span class=k>while</span> <span class=p>(</span><span class=n>curr_iter</span> <span class=o>&lt;</span> <span class=n>max_iterations</span><span class=p>):</span>
</span></span><span class=line><span class=cl>        <span class=n>mistakes</span> <span class=o>=</span> <span class=mi>0</span>
</span></span><span class=line><span class=cl>        <span class=n>curr_iter</span> <span class=o>+=</span> <span class=mi>1</span>
</span></span><span class=line><span class=cl>        <span class=k>for</span> <span class=n>i</span> <span class=ow>in</span> <span class=nb>range</span><span class=p>(</span><span class=n>n</span><span class=p>):</span>
</span></span><span class=line><span class=cl>            <span class=n>yhat</span> <span class=o>=</span> <span class=o>-</span><span class=mi>1</span> <span class=k>if</span> <span class=n>np</span><span class=o>.</span><span class=n>dot</span><span class=p>(</span><span class=n>w</span><span class=p>,</span> <span class=n>X</span><span class=p>[</span><span class=n>i</span><span class=p>])</span> <span class=o>&lt;=</span> <span class=mi>0</span> <span class=k>else</span> <span class=mi>1</span>
</span></span><span class=line><span class=cl>            <span class=k>if</span> <span class=p>(</span><span class=n>yhat</span> <span class=o>!=</span> <span class=n>y</span><span class=p>[</span><span class=n>i</span><span class=p>]):</span>
</span></span><span class=line><span class=cl>                <span class=n>mistakes</span> <span class=o>+=</span> <span class=mi>1</span>
</span></span><span class=line><span class=cl>                <span class=n>w</span> <span class=o>+=</span> <span class=n>y</span><span class=p>[</span><span class=n>i</span><span class=p>]</span><span class=o>*</span><span class=n>X</span><span class=p>[</span><span class=n>i</span><span class=p>]</span>
</span></span><span class=line><span class=cl>        <span class=c1># if a pass contains no mistakes then we are done!</span>
</span></span><span class=line><span class=cl>        <span class=k>if</span> <span class=n>mistakes</span> <span class=o>==</span> <span class=mi>0</span><span class=p>:</span>
</span></span><span class=line><span class=cl>            <span class=k>break</span>
</span></span><span class=line><span class=cl>
</span></span><span class=line><span class=cl>    <span class=c1>#return the normal to the plane and bias   </span>
</span></span><span class=line><span class=cl>    <span class=k>return</span> <span class=n>w</span><span class=p>[:</span><span class=o>-</span><span class=mi>1</span><span class=p>],</span> <span class=n>w</span><span class=p>[</span><span class=o>-</span><span class=mi>1</span><span class=p>]</span>
</span></span></code></pre></td></tr></table></div></div><p>Once we found the weights using the update rule on the training data we can use them to predict new test instances by using the dot product.</p><div class=highlight><div class=chroma><table class=lntable><tr><td class=lntd><pre tabindex=0 class=chroma><code><span class=lnt>1
</span><span class=lnt>2
</span><span class=lnt>3
</span><span class=lnt>4
</span><span class=lnt>5
</span><span class=lnt>6
</span></code></pre></td><td class=lntd><pre tabindex=0 class=chroma><code class=language-python data-lang=python><span class=line><span class=cl><span class=k>def</span> <span class=nf>predict</span><span class=p>(</span><span class=n>x</span><span class=p>,</span> <span class=n>w</span><span class=p>,</span> <span class=n>b</span><span class=p>):</span>
</span></span><span class=line><span class=cl>    <span class=n>yhat</span> <span class=o>=</span> <span class=n>np</span><span class=o>.</span><span class=n>dot</span><span class=p>(</span><span class=n>w</span><span class=p>,</span> <span class=n>x</span><span class=p>)</span> <span class=o>+</span> <span class=n>b</span>
</span></span><span class=line><span class=cl>    <span class=k>if</span> <span class=n>yhat</span> <span class=o>&gt;</span> <span class=mi>0</span><span class=p>:</span>
</span></span><span class=line><span class=cl>        <span class=k>return</span> <span class=mi>1</span>
</span></span><span class=line><span class=cl>    <span class=k>else</span><span class=p>:</span>
</span></span><span class=line><span class=cl>        <span class=k>return</span> <span class=o>-</span><span class=mi>1</span>
</span></span></code></pre></td></tr></table></div></div><h3 id=conclusion>Conclusion</h3><p>In this post, we looked at the perceptron algorithm. We then looked at the Perceptron Update Rule and visualized some examples. We also discussed when the algorithm will succeed. i.e., when the data is linearly separable.</p></div><footer class=post-footer><nav class=post-nav><a class=prev href=/post/the-naive-bayes-assumption/><i class="iconfont icon-left"></i>
<span class="prev-text nav-default">The Naive Bayes Assumtion</span>
<span class="prev-text nav-mobile">Prev</span></a></nav></footer></article></div></div></main><footer id=footer class=footer><div class=copyright>Disclaimer: The posts on this site are my own and do not represent my employer's view in any way.<div><br><div class=social-links><a href=https://github.com/MostafaEissa class="iconfont icon-github" title=github></a></div><div class=copyright><span class=power-by>Powered by <a class=hexo-link href=https://gohugo.io>Hugo</a>
</span><span class=division>|</span>
<span class=theme-info>Theme -
<a class=theme-link href=https://github.com/olOwOlo/hugo-theme-even>Even</a>
</span><span class=copyright-year>&copy;
2019 -
2024<span class=heart><i class="iconfont icon-heart"></i></span><span>Mostafa Abdelrahman</span></span></div></footer><div class=back-to-top id=back-to-top><i class="iconfont icon-up"></i></div></div><script type=text/javascript src=/lib/jquery/jquery-3.2.1.min.js></script><script type=text/javascript src=/lib/slideout/slideout-1.0.1.min.js></script><script type=text/javascript src=/lib/fancybox/jquery.fancybox-3.1.20.min.js></script><script type=text/javascript src=/js/main.min.1821caf6efbc14cf4fb96fdb63c79a3babde5deb1fb600e97b9a2ced5678a2ed.js></script><script type=text/javascript>window.MathJax={tex:{inlineMath:[["$","$"],["\\(","\\)"]]}}</script><script async src=https://cdn.jsdelivr.net/npm/mathjax@3.0.5/es5/tex-mml-chtml.js integrity="sha256-HGLuEfFcsUJGhvB8cQ8nr0gai9EucOOaIxFw7qxmd+w=" crossorigin=anonymous></script></body></html>