<!doctype html><html lang=en-us><head><meta charset=UTF-8><meta name=viewport content="width=device-width,initial-scale=1"><meta content name=keywords><meta content="Mostafa Abdelrahman" name=author><meta property="og:title" content="The Perceptron - Beyond Lambda"><meta property="og:url" content="https://mostafaeissa.github.io/post/the-perceptron/"><meta property="og:description" content><meta property="og:type" content="website"><title>The Perceptron | Beyond Lambda</title>
<link rel=stylesheet href=https://mostafaeissa.github.io//css/style.css><link rel=stylesheet href=//cdnjs.cloudflare.com/ajax/libs/font-awesome/4.7.0/css/font-awesome.min.css integrity="sha256-eZrrJcwDc/3uDhsdt61sL2oOBY362qM3lon1gyExkL0=" crossorigin=anonymous><link rel=stylesheet href=//cdn.jsdelivr.net/gh/highlightjs/cdn-release@10.2.0/build/styles/default.min.css><script type=text/javascript>window.MathJax={tex:{inlineMath:[["$","$"],["\\(","\\)"]]}}</script><script async src=https://cdn.jsdelivr.net/npm/mathjax@3.0.5/es5/tex-mml-chtml.js integrity="sha256-HGLuEfFcsUJGhvB8cQ8nr0gai9EucOOaIxFw7qxmd+w=" crossorigin=anonymous></script></head><body><section class=section><div class=container><nav class=nav><img src=/android-chrome-512x512.png alt=Avatar style=margin-right:1em height=100px><div class=nav-left style=flex-basis:auto><a class=nav-item href=https://mostafaeissa.github.io/><h1 class="title is-4">Beyond Lambda</h1></a><nav class="nav-item level is-mobile"><a class=level-item href=/tags>tags
</a><a class=level-item href=https://mostafaeissa.github.io/about/>about</a></nav></div><div class=nav-right><nav class="nav-item level is-mobile"><a class=level-item href=https://github.com/MostafaEissa target=_blank><span class=icon><i class="fa fa-github"></i>
</span></a><a class=level-item href=https://linkedin.com/in/mostafa-abdelrahman-16356b12a target=_blank><span class=icon><i class="fa fa-linkedin-square"></i>
</span></a><a class=level-item href=https://mostafaeissa.github.io/index.xml target=_blank><span class=icon><i class="fa fa-rss"></i></span></a></nav></div></nav></div></section><section class=section><div class=container><h1 class=title>The Perceptron</h1><h2 class="subtitle is-5">February 26, 2020 by Mostafa Abdelrahman</h2><div class=tags><a class="button is-link" href=https://mostafaeissa.github.io/tags/machine-learning>Machine Learning</a></div><div class=content><p>The perceptron is a learning algorithm. A rather simple one yet surprising, it can acheive very good results as we will explore in this post.</p><h3 id=introduction>Introduction</h3><p>The perceptron is used for classification task where there are two categories and the objective is to learn how to differentiate between them. For example, whether an email is spam? Or whether a house will sell over a certain asking price?</p><p>Traditionally, for each object we want to classify a number of features are collected that best describe the object. In the house example mentioned earlier, the features might be the number of rooms, total area, number of bathrooms, etc. As a result we end up with a number of numerical features describing each object (instance) (Note: categorical features can be converted to numerical using one hot encoding) and our task can be described mathematically as given input instances $X$ of size $n$ where each instance is $d$-dimensional vector $a_1,a_2,a_3,….,a_d$ containing the feature values, design a function $h(x)$ that takes an input instance x and outputs $+1$ if $x$ belongs to the positive classs and $-1$ otherwise.</p><p>$$
\begin{equation}
h(x) =
\begin{cases}
+1 & if \enspace x \in \text{positive class} \newline
-1 & if \enspace x \in \text{negative class }
\end{cases}
\end{equation}
$$</p><p>In other words, each input is represented as a vector where each individual component is one of the features and the perceptron is trying to find a hyperplane (think line in 2D) to separate the instances of the $+1$ class from the instances of $-1$ class.</p><p>For simplicity, let us focus on the 2D plane and limit inputs to only two features $a_1$ and $a_2$. In this setting, each input corresponds to a point in 2D as seen in the next figure. The goal of the perceptron algorithm is to find a line such that all points on one side belong to the positive class and all points on the other side belong to the negative class.</p><p><img src=/images/the-perceptron/the-separating-hyperplane.png alt="the separating hyperplane"></p><p>A plane can be described by a vector $w$ that is normal to the plane. Hence, when we say we want to find a separating plane we mean that we want to find the components of the vector $w$ that describe the plane.</p><p>Once the perceptron algorithm has found such hyperplane, we can classify instances based on the following rule. Note that the formula contains a bias term $b$ without this term, the hyperplane will always have to go through the origin</p><p>$$
\begin{equation}
h(x)=
\begin{cases}
+1 & if \enspace w.x+b > 0 \newline
-1 & if \enspace w.x+b &lt; 0
\end{cases}
\end{equation}$$</p><p>Here, $w.x$ is the dot product which is the sum component wise components of both vectors $\sum\limits_{i=1}^{d} w_i x_i$</p><h3 id=the-perceptron-update-rule>The Perceptron Update Rule</h3><p>The question now is how can we find such $w$ that classifies the points correctly. A simple idea is to initialize the $w$ vector randomly and go over the points one by one. If the point is correctly classified then we do not need to do anything however, if is misclassified we need to move the hyperplane (update $w$) so that the point will eventually become correctly classified. A good choice, as shown in the next figure, is a vector that is proportional to the misclassified data point but in the opposite direction i.e. $w_{new} =w_{old}+ηyx$. The algorithm keeps iterating until $w$ converges does not change anymore.</p><p><img src=/images/the-perceptron/the-perceptron-update-rule.png alt="the perceptron update rule"></p><p>Each iteration of the algorithm consists of a full pass over all the training examples. The algorithm terminates when in a given iteration no point was misclassified.
A natural question at this point is how can we update the bias term $b$ as well. The trick is to augment our training data with all ones vector and treat the bias as another weight component (i.e. weight component of a feature whose value is alaways one).</p><p>$$
\begin{aligned}
h(x) &= w.x+b \newline
&= ∑<em>{i=1}^d w_i x_i+b \newline
&= ∑</em>{i=1}^d w_i x_i+b*1 \newline
&= ∑_{i=1}^{d+1} w_i x_i
\end{aligned}
$$</p><p>where $x_{d+1}$ always equals one and $w_{d+1}$ is the bias term</p><h3 id=worked-example>Worked Example</h3><p>To Understand the algorithm better, let’s go through a toy example. Suppose we have the following positive and negative examples in 3D</p><table><thead><tr><th style=text-align:center>Positive Examples</th><th style=text-align:center>Negative Examples</th></tr></thead><tbody><tr><td style=text-align:center>$[0 \enspace 0 \enspace 0]$</td><td style=text-align:center>$[0 \enspace 1 \enspace 1]$</td></tr><tr><td style=text-align:center>$[0 \enspace 0 \enspace 1]$</td><td style=text-align:center>$[0 \enspace 1 \enspace 0]$</td></tr><tr><td style=text-align:center>$[1 \enspace 0 \enspace 1]$</td><td style=text-align:center>$[1 \enspace 1 \enspace 0]$</td></tr><tr><td style=text-align:center>$[1 \enspace 0 \enspace 0]$</td><td style=text-align:center>$[1 \enspace 1 \enspace 1]$</td></tr></tbody></table><p><img src=/images/the-perceptron/3d-example.png alt="3d example"></p><p><em><strong>Iteration 1</strong></em></p><table><thead><tr><th style=text-align:center>Weight $[w_0 \enspace w_1 \enspace w_2 \enspace b]$</th><th style=text-align:center>Data $[x_0 \enspace x_1 \enspace x_2 \enspace 1]$</th><th style=text-align:right><strong>${w.x}$</strong></th><th style=text-align:left>Label</th><th>Comment</th><th style=text-align:center>New weight</th></tr></thead><tbody><tr><td style=text-align:center>$[0 \enspace 0\enspace 0\enspace 0]$</td><td style=text-align:center>$[0 \enspace 0 \enspace 0 \enspace 1]$</td><td style=text-align:right>$0 \leq 0$</td><td style=text-align:left>$+$</td><td>Wrong. Add sample</td><td style=text-align:center>$[0 \enspace 0 \enspace 0 \enspace 1]$</td></tr><tr><td style=text-align:center>$[0 \enspace 0 \enspace 0 \enspace 1]$</td><td style=text-align:center>$[0 \enspace 0 \enspace 1 \enspace 1]$</td><td style=text-align:right>$1 > 0$</td><td style=text-align:left>$+$</td><td>OK</td><td style=text-align:center>No Change</td></tr><tr><td style=text-align:center>$[0 \enspace 0 \enspace 0 \enspace 1]$</td><td style=text-align:center>$[0 \enspace 1 \enspace 0 \enspace 1]$</td><td style=text-align:right>$1 > 0$</td><td style=text-align:left>$-$</td><td>Wrong. Subtract sample</td><td style=text-align:center>$[0 \enspace {-1} \enspace 0 \enspace 0]$</td></tr><tr><td style=text-align:center>$[0 \enspace {-1} \enspace 0 \enspace 0]$</td><td style=text-align:center>$[0 \enspace 1 \enspace 1 \enspace 1]$</td><td style=text-align:right>${-1} &lt; 0$</td><td style=text-align:left>$-$</td><td>OK</td><td style=text-align:center>No Change</td></tr><tr><td style=text-align:center>$[0 \enspace {-1} \enspace 0 \enspace 0]$</td><td style=text-align:center>$[1 \enspace 0 \enspace 0 \enspace 1]$</td><td style=text-align:right>$0 \leq 0$</td><td style=text-align:left>$+$</td><td>Wrong. Add sample</td><td style=text-align:center>$[1 \enspace {-1} \enspace 0 \enspace 1]$</td></tr><tr><td style=text-align:center>$[1 \enspace {-1} \enspace 0 \enspace 1]$</td><td style=text-align:center>$[1 \enspace 0 \enspace 1 \enspace 1]$</td><td style=text-align:right>$2 > 0$</td><td style=text-align:left>$+$</td><td>OK</td><td style=text-align:center>No Change</td></tr><tr><td style=text-align:center>$[1 \enspace {-1} \enspace 0 \enspace 1]$</td><td style=text-align:center>$[1 \enspace 1 \enspace 0 \enspace 1]$</td><td style=text-align:right>$1 > 0$</td><td style=text-align:left>$-$</td><td>Wrong. Subtract sample</td><td style=text-align:center>$[0 \enspace {-2} \enspace 0 \enspace 0]$</td></tr><tr><td style=text-align:center>$[0 \enspace {-2} \enspace 0 \enspace 0]$</td><td style=text-align:center>$[1 \enspace 1 \enspace 1 \enspace 1]$</td><td style=text-align:right>${-2} &lt; 0$</td><td style=text-align:left>$-$</td><td>OK</td><td style=text-align:center>No Change</td></tr></tbody></table><p><em><strong>Iteration 2</strong></em></p><table><thead><tr><th style=text-align:center>Weight $[w_0 \enspace w_1 \enspace w_2 \enspace b]$</th><th style=text-align:center>Data $[x_0 \enspace x_1 \enspace x_2 \enspace 1]$</th><th style=text-align:right><strong>${w.x}$</strong></th><th style=text-align:left>Label</th><th>Comment</th><th style=text-align:center>New weight</th></tr></thead><tbody><tr><td style=text-align:center>$[0 \enspace {-2}\enspace 0\enspace 0]$</td><td style=text-align:center>$[0 \enspace 0 \enspace 0 \enspace 1]$</td><td style=text-align:right>$0 \leq 0$</td><td style=text-align:left>$+$</td><td>Wrong. Add sample</td><td style=text-align:center>$[0 \enspace {-2} \enspace 0 \enspace 1]$</td></tr><tr><td style=text-align:center>$[0 \enspace {-2} \enspace 0 \enspace 1]$</td><td style=text-align:center>$[0 \enspace 0 \enspace 1 \enspace 1]$</td><td style=text-align:right>$1 > 0$</td><td style=text-align:left>$+$</td><td>OK</td><td style=text-align:center>No Change</td></tr><tr><td style=text-align:center>$[0 \enspace {-2} \enspace 0 \enspace 1]$</td><td style=text-align:center>$[0 \enspace 1 \enspace 0 \enspace 1]$</td><td style=text-align:right>${-1} &lt; 0$</td><td style=text-align:left>$-$</td><td>OK</td><td style=text-align:center>No Change</td></tr><tr><td style=text-align:center>$[0 \enspace {-2} \enspace 0 \enspace 1]$</td><td style=text-align:center>$[0 \enspace 1 \enspace 1 \enspace 1]$</td><td style=text-align:right>${-1} &lt; 0$</td><td style=text-align:left>$-$</td><td>OK</td><td style=text-align:center>No Change</td></tr><tr><td style=text-align:center>$[0 \enspace {-2} \enspace 0 \enspace 1]$</td><td style=text-align:center>$[1 \enspace 0 \enspace 0 \enspace 1]$</td><td style=text-align:right>$1 > 0$</td><td style=text-align:left>$+$</td><td>OK</td><td style=text-align:center>No Change</td></tr><tr><td style=text-align:center>$[0 \enspace {-2} \enspace 0 \enspace 1]$</td><td style=text-align:center>$[1 \enspace 0 \enspace 1 \enspace 1]$</td><td style=text-align:right>$1 > 0$</td><td style=text-align:left>$+$</td><td>OK</td><td style=text-align:center>No Change</td></tr><tr><td style=text-align:center>$[0 \enspace {-2} \enspace 0 \enspace 1]$</td><td style=text-align:center>$[1 \enspace 1 \enspace 0 \enspace 1]$</td><td style=text-align:right>${-1} &lt; 0$</td><td style=text-align:left>$-$</td><td>OK</td><td style=text-align:center>No Change</td></tr><tr><td style=text-align:center>$[0 \enspace {-2} \enspace 0 \enspace 1]$</td><td style=text-align:center>$[1 \enspace 1 \enspace 1 \enspace 1]$</td><td style=text-align:right>${-1} &lt; 0$</td><td style=text-align:left>$-$</td><td>OK</td><td style=text-align:center>No Change</td></tr></tbody></table><p><em><strong>Iteration 3</strong></em></p><table><thead><tr><th style=text-align:center>Weight $[w_0 \enspace w_1 \enspace w_2 \enspace b]$</th><th style=text-align:center>Data $[x_0 \enspace x_1 \enspace x_2 \enspace 1]$</th><th style=text-align:right><strong>${w.x}$</strong></th><th style=text-align:left>Label</th><th>Comment</th><th style=text-align:center>New weight</th></tr></thead><tbody><tr><td style=text-align:center>$[0 \enspace {-2}\enspace 0\enspace 1]$</td><td style=text-align:center>$[0 \enspace 0 \enspace 0 \enspace 1]$</td><td style=text-align:right>$1 > 0$</td><td style=text-align:left>$+$</td><td>OK</td><td style=text-align:center>No Change</td></tr><tr><td style=text-align:center>$[0 \enspace {-2} \enspace 0 \enspace 1]$</td><td style=text-align:center>$[0 \enspace 0 \enspace 1 \enspace 1]$</td><td style=text-align:right>$1 > 0$</td><td style=text-align:left>$+$</td><td>OK</td><td style=text-align:center>No Change</td></tr><tr><td style=text-align:center>$[0 \enspace {-2} \enspace 0 \enspace 1]$</td><td style=text-align:center>$[0 \enspace 1 \enspace 0 \enspace 1]$</td><td style=text-align:right>${-1} &lt; 0$</td><td style=text-align:left>$-$</td><td>OK</td><td style=text-align:center>No Change</td></tr><tr><td style=text-align:center>$[0 \enspace {-2} \enspace 0 \enspace 1]$</td><td style=text-align:center>$[0 \enspace 1 \enspace 1 \enspace 1]$</td><td style=text-align:right>${-1} &lt; 0$</td><td style=text-align:left>$-$</td><td>OK</td><td style=text-align:center>No Change</td></tr><tr><td style=text-align:center>$[0 \enspace {-2} \enspace 0 \enspace 1]$</td><td style=text-align:center>$[1 \enspace 0 \enspace 0 \enspace 1]$</td><td style=text-align:right>$1 > 0$</td><td style=text-align:left>$+$</td><td>OK</td><td style=text-align:center>No Change</td></tr><tr><td style=text-align:center>$[0 \enspace {-2} \enspace 0 \enspace 1]$</td><td style=text-align:center>$[1 \enspace 0 \enspace 1 \enspace 1]$</td><td style=text-align:right>$1 > 0$</td><td style=text-align:left>$+$</td><td>OK</td><td style=text-align:center>No Change</td></tr><tr><td style=text-align:center>$[0 \enspace {-2} \enspace 0 \enspace 1]$</td><td style=text-align:center>$[1 \enspace 1 \enspace 0 \enspace 1]$</td><td style=text-align:right>${-1} &lt; 0$</td><td style=text-align:left>$-$</td><td>OK</td><td style=text-align:center>No Change</td></tr><tr><td style=text-align:center>$[0 \enspace {-2} \enspace 0 \enspace 1]$</td><td style=text-align:center>$[1 \enspace 1 \enspace 1 \enspace 1]$</td><td style=text-align:right>${-1} &lt; 0$</td><td style=text-align:left>$-$</td><td>OK</td><td style=text-align:center>No Change</td></tr></tbody></table><p>So, the hyperplane found by the perceptron algorithm can be $[0 \enspace {-2} \enspace 0 \enspace 1]$</p><p><img src=/images/the-perceptron/3d-example-solved.png alt="3d example solved"></p><h3 id=limitations-xor-example>Limitations: XOR Example</h3><p>However, there is one caveat for the Perceptron algorithm. The data needs to be linearly separable otherwise, the perceptron algorithm will fail to find a solution. To illustrate this point let’s consider the case of the XOR function. The XOR function outputs a 1 if only one of the inputs is one, otherwise it outputs zero.</p><table><thead><tr><th style=text-align:center>Input 1</th><th style=text-align:center>Input 2</th><th style=text-align:center>Output</th></tr></thead><tbody><tr><td style=text-align:center>0</td><td style=text-align:center>0</td><td style=text-align:center>0</td></tr><tr><td style=text-align:center>0</td><td style=text-align:center>1</td><td style=text-align:center>1</td></tr><tr><td style=text-align:center>1</td><td style=text-align:center>0</td><td style=text-align:center>1</td></tr><tr><td style=text-align:center>1</td><td style=text-align:center>1</td><td style=text-align:center>0</td></tr></tbody></table><p>We can visualize the XOR output in the 2D space as shown below. There is no line that can separate the positive from the negative points which means the data is not linearly separable.</p><p><img src=/images/the-perceptron/XOR.png alt="XOR in 2D"></p><p>If we try to follow the same update rule specified earlier we will get stuck in an infinite loop.</p><p><em><strong>Iteration 1</strong></em></p><table><thead><tr><th style=text-align:center>Weight $[w_0 \enspace w_1 \enspace b]$</th><th style=text-align:center>Data $[x_0 \enspace x_1 \enspace 1]$</th><th style=text-align:right><strong>${w.x}$</strong></th><th style=text-align:left>Label</th><th>Comment</th><th style=text-align:center>New weight</th></tr></thead><tbody><tr><td style=text-align:center>$[0 \enspace 0\enspace 0]$</td><td style=text-align:center>$[0 \enspace 0 \enspace 1]$</td><td style=text-align:right>$0 \leq 0$</td><td style=text-align:left>$-$</td><td>OK</td><td style=text-align:center>No Change</td></tr><tr><td style=text-align:center>$[0 \enspace 0\enspace 0]$</td><td style=text-align:center>$[0 \enspace 1 \enspace 1]$</td><td style=text-align:right>$0 \leq 0$</td><td style=text-align:left>$+$</td><td>Wrong. Add sample</td><td style=text-align:center>$[0 \enspace 1\enspace 1]$</td></tr><tr><td style=text-align:center>$[0 \enspace 1\enspace 1]$</td><td style=text-align:center>$[1 \enspace 1 \enspace 1]$</td><td style=text-align:right>$2 > 0$</td><td style=text-align:left>$-$</td><td>Wrong. Subtract sample</td><td style=text-align:center>$[{-1} \enspace 0\enspace 0]$</td></tr><tr><td style=text-align:center>$[{-1} \enspace 0\enspace 0]$</td><td style=text-align:center>$[1 \enspace 0 \enspace 1]$</td><td style=text-align:right>${-1} &lt; 0$</td><td style=text-align:left>$+$</td><td>Wrong. Add sample</td><td style=text-align:center>$[0 \enspace 0\enspace 1]$</td></tr></tbody></table><p><em><strong>Iteration 2</strong></em></p><table><thead><tr><th style=text-align:center>Weight $[w_0 \enspace w_1 \enspace b]$</th><th style=text-align:center>Data $[x_0 \enspace x_1 \enspace 1]$</th><th style=text-align:right><strong>${w.x}$</strong></th><th style=text-align:left>Label</th><th>Comment</th><th style=text-align:center>New weight</th></tr></thead><tbody><tr><td style=text-align:center>$[0 \enspace 0\enspace 1]$</td><td style=text-align:center>$[0 \enspace 0 \enspace 1]$</td><td style=text-align:right>$1 > 0$</td><td style=text-align:left>$-$</td><td>Wrong. Subtract sample</td><td style=text-align:center>$[0 \enspace 0\enspace 0]$</td></tr><tr><td style=text-align:center>$[0 \enspace 0\enspace 0]$</td><td style=text-align:center>$[0 \enspace 1 \enspace 1]$</td><td style=text-align:right>$0 \leq 0$</td><td style=text-align:left>$+$</td><td>Wrong. Add sample</td><td style=text-align:center>$[0 \enspace 1\enspace 1]$</td></tr><tr><td style=text-align:center>$[0 \enspace 1\enspace 1]$</td><td style=text-align:center>$[1 \enspace 1 \enspace 1]$</td><td style=text-align:right>$2 > 0$</td><td style=text-align:left>$-$</td><td>Wrong. Subtract sample</td><td style=text-align:center>$[{-1} \enspace 0\enspace 0]$</td></tr><tr><td style=text-align:center>$[{-1} \enspace 0\enspace 0]$</td><td style=text-align:center>$[1 \enspace 0 \enspace 1]$</td><td style=text-align:right>${-1} &lt; 0$</td><td style=text-align:left>$+$</td><td>Wrong. Add sample</td><td style=text-align:center>$[0 \enspace 0\enspace 1]$</td></tr></tbody></table><p>And we are back to the same weight that we started the seconditeration with and we become stuck in the same pattern no matter how many passes we make over the data.</p><h3 id=python-implementation>Python Implementation</h3><p>We can easily implement the above update rule in a couple of lines of python.</p><div class=highlight><pre tabindex=0 style=color:#f8f8f2;background-color:#272822;-moz-tab-size:4;-o-tab-size:4;tab-size:4><code class=language-python data-lang=python><span style=display:flex><span><span style=color:#66d9ef>def</span> <span style=color:#a6e22e>perceptron</span>(X, y):
</span></span><span style=display:flex><span>    <span style=color:#75715e># append ones to input for bias</span>
</span></span><span style=display:flex><span>    X <span style=color:#f92672>=</span> np<span style=color:#f92672>.</span>hstack((X, np<span style=color:#f92672>.</span>ones((X<span style=color:#f92672>.</span>shape[<span style=color:#ae81ff>0</span>],<span style=color:#ae81ff>1</span>))))
</span></span><span style=display:flex><span>
</span></span><span style=display:flex><span>    <span style=color:#75715e>#limit to number of iterations to do before giving up</span>
</span></span><span style=display:flex><span>    max_iterations <span style=color:#f92672>=</span> <span style=color:#ae81ff>1000</span>
</span></span><span style=display:flex><span>    curr_iter <span style=color:#f92672>=</span> <span style=color:#ae81ff>0</span>
</span></span><span style=display:flex><span>
</span></span><span style=display:flex><span>    n,d <span style=color:#f92672>=</span> X<span style=color:#f92672>.</span>shape
</span></span><span style=display:flex><span>    
</span></span><span style=display:flex><span>    <span style=color:#75715e># initialize w to all zero vector</span>
</span></span><span style=display:flex><span>    w <span style=color:#f92672>=</span> np<span style=color:#f92672>.</span>zeros(d)
</span></span><span style=display:flex><span>
</span></span><span style=display:flex><span>    <span style=color:#66d9ef>while</span> (curr_iter <span style=color:#f92672>&lt;</span> max_iterations):
</span></span><span style=display:flex><span>        mistakes <span style=color:#f92672>=</span> <span style=color:#ae81ff>0</span>
</span></span><span style=display:flex><span>        curr_iter <span style=color:#f92672>+=</span> <span style=color:#ae81ff>1</span>
</span></span><span style=display:flex><span>        <span style=color:#66d9ef>for</span> i <span style=color:#f92672>in</span> range(n):
</span></span><span style=display:flex><span>            yhat <span style=color:#f92672>=</span> <span style=color:#f92672>-</span><span style=color:#ae81ff>1</span> <span style=color:#66d9ef>if</span> np<span style=color:#f92672>.</span>dot(w, X[i]) <span style=color:#f92672>&lt;=</span> <span style=color:#ae81ff>0</span> <span style=color:#66d9ef>else</span> <span style=color:#ae81ff>1</span>
</span></span><span style=display:flex><span>            <span style=color:#66d9ef>if</span> (yhat <span style=color:#f92672>!=</span> y[i]):
</span></span><span style=display:flex><span>                mistakes <span style=color:#f92672>+=</span> <span style=color:#ae81ff>1</span>
</span></span><span style=display:flex><span>                w <span style=color:#f92672>+=</span> y[i]<span style=color:#f92672>*</span>X[i]
</span></span><span style=display:flex><span>        <span style=color:#75715e># if a pass contains no mistakes then we are done!</span>
</span></span><span style=display:flex><span>        <span style=color:#66d9ef>if</span> mistakes <span style=color:#f92672>==</span> <span style=color:#ae81ff>0</span>:
</span></span><span style=display:flex><span>            <span style=color:#66d9ef>break</span>
</span></span><span style=display:flex><span>
</span></span><span style=display:flex><span>    <span style=color:#75715e>#return the normal to the plane and bias   </span>
</span></span><span style=display:flex><span>    <span style=color:#66d9ef>return</span> w[:<span style=color:#f92672>-</span><span style=color:#ae81ff>1</span>], w[<span style=color:#f92672>-</span><span style=color:#ae81ff>1</span>]
</span></span></code></pre></div><p>Once we found the weights using the update rule on the training data we can use them to predict new test instances by using the dot product.</p><div class=highlight><pre tabindex=0 style=color:#f8f8f2;background-color:#272822;-moz-tab-size:4;-o-tab-size:4;tab-size:4><code class=language-python data-lang=python><span style=display:flex><span><span style=color:#66d9ef>def</span> <span style=color:#a6e22e>predict</span>(x, w, b):
</span></span><span style=display:flex><span>    yhat <span style=color:#f92672>=</span> np<span style=color:#f92672>.</span>dot(w, x) <span style=color:#f92672>+</span> b
</span></span><span style=display:flex><span>    <span style=color:#66d9ef>if</span> yhat <span style=color:#f92672>&gt;</span> <span style=color:#ae81ff>0</span>:
</span></span><span style=display:flex><span>        <span style=color:#66d9ef>return</span> <span style=color:#ae81ff>1</span>
</span></span><span style=display:flex><span>    <span style=color:#66d9ef>else</span>:
</span></span><span style=display:flex><span>        <span style=color:#66d9ef>return</span> <span style=color:#f92672>-</span><span style=color:#ae81ff>1</span>
</span></span></code></pre></div><h3 id=conclusion>Conclusion</h3><p>In this post, we looked at the perceptron algorithm. We then looked at the Perceptron Update Rule and visualized some examples. We also discussed when the algorithm will succeed. i.e., when the data is linearly separable.</p></div></div></section><section class=section><div class="container has-text-centered"><p></p></div></section><script src=//cdn.jsdelivr.net/gh/highlightjs/cdn-release@10.2.0/build/highlight.min.js></script><script>hljs.initHighlightingOnLoad()</script></body>