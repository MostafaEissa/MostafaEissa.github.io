<!doctype html><html lang=en-us><head><meta charset=UTF-8><meta name=viewport content="width=device-width,initial-scale=1"><meta content name=keywords><meta content="Mostafa Abdelrahman" name=author><meta property="og:title" content="K-means clustering - Beyond Lambda"><meta property="og:url" content="https://mostafaeissa.github.io/post/k-means-clustering/"><meta property="og:description" content><meta property="og:type" content="website"><title>K-means clustering | Beyond Lambda</title>
<link rel=stylesheet href=https://mostafaeissa.github.io//css/style.css><link rel=stylesheet href=//cdnjs.cloudflare.com/ajax/libs/font-awesome/4.7.0/css/font-awesome.min.css integrity="sha256-eZrrJcwDc/3uDhsdt61sL2oOBY362qM3lon1gyExkL0=" crossorigin=anonymous><link rel=stylesheet href=//cdn.jsdelivr.net/gh/highlightjs/cdn-release@10.2.0/build/styles/default.min.css><script type=text/javascript>window.MathJax={tex:{inlineMath:[["$","$"],["\\(","\\)"]]}}</script><script async src=https://cdn.jsdelivr.net/npm/mathjax@3.0.5/es5/tex-mml-chtml.js integrity="sha256-HGLuEfFcsUJGhvB8cQ8nr0gai9EucOOaIxFw7qxmd+w=" crossorigin=anonymous></script></head><body><section class=section><div class=container><nav class=nav><img src=/android-chrome-512x512.png alt=Avatar style=margin-right:1em height=100px><div class=nav-left style=flex-basis:auto><a class=nav-item href=https://mostafaeissa.github.io/><h1 class="title is-4">Beyond Lambda</h1></a><nav class="nav-item level is-mobile"><a class=level-item href=/tags>tags
</a><a class=level-item href=https://mostafaeissa.github.io/about/>about</a></nav></div><div class=nav-right><nav class="nav-item level is-mobile"><a class=level-item href=https://github.com/MostafaEissa target=_blank><span class=icon><i class="fa fa-github"></i>
</span></a><a class=level-item href=https://linkedin.com/in/mostafa-abdelrahman-16356b12a target=_blank><span class=icon><i class="fa fa-linkedin-square"></i>
</span></a><a class=level-item href=https://mostafaeissa.github.io/index.xml target=_blank><span class=icon><i class="fa fa-rss"></i></span></a></nav></div></nav></div></section><section class=section><div class=container><h1 class=title>K-means clustering</h1><h2 class="subtitle is-5">October 18, 2021 by Mostafa Abdelrahman</h2><div class=tags><a class="button is-link" href=https://mostafaeissa.github.io/tags/machine-learning>Machine Learning</a></div><div class=content><p>K-means is a clustering algorithm that is relatively old yet it still in use today because of its simplicity and power.</p><h3 id=introduction>Introduction</h3><p>Unlike supervised learning algorithm, in unsupervised learning algorithms there is no notion of a label. We are simply given a set of input data and we want to conclude something from it. One idea is clustering. In this post we will focus on K-means algorithm.</p><p>In layman terms, clustering is grouping a collection of unlabeled data into similar sets. The resulting clusters will be vastly affected by how you decide if two points are similar to each other. Perhaps the simplest and most obvious measure of similarity is proximity where close by data points should belong to the same cluster.</p><p>K-means applies this idea in an iterative way, it starts with a random set of k-points $m_1, m_2, …, m_k$ that act as the centroid of the $k$ clusters to be determined (hence the name k-means) and it proceeds in two steps:</p><ul><li><p><em>Assignment step</em>: each point in the data set is assigned to the closest cluster based on the Euclidean distance to the cluster centroid. Mathematically speaking, the point $x$ is assigned to the cluster $C_i$ so that each cluster has a set of points $S_i$ such that $\lvert\lvert {x – m_i} \rvert\rvert \leq \lvert\lvert x – m_j \rvert\rvert \enspace \forall \enspace 1 \leq j \leq k$</p></li><li><p><em>Update step</em>: recalculate the cluster centroid based on the new assigned points $m_i=\frac{1}{\rvert S_i \lvert}\sum \limits_{x_j \in S_i} x_j$</p></li></ul><p>The algorithm keeps iterating until no more changes happen in data point assignment and centroid locations.</p><h3 id=worked-example>Worked Example</h3><p>Suppose we are given the following dataset in $R_2$ and we want to cluster the data into k=2 clusters.</p><p><img src=/images/k-means-clustering/example-iteration1.png alt="k-means example iteration 1"></p><p>The first step is to pick 2 random initial locations for the cluster centroids. Let $m_1 = (0, 0.5)$ and $m_2 = (0.5, 0.5)$.</p><p><strong>Iteration 1:</strong></p><p><em>Assignment step:</em></p><table><thead><tr><th style=text-align:left>Distance to Centroid</th><th style=text-align:left>$m1 (0, 0.5)$</th><th style=text-align:left>$m2 (0.5, 0.5)$</th></tr></thead><tbody><tr><td style=text-align:left>$P1 (0, 0)$</td><td style=text-align:left>$\sqrt{(0-0)^2+(0-0.5)^2}=$ <strong>$0.5$</strong></td><td style=text-align:left>$\sqrt{(0-0.5)^2+(0-0.5)^2}=0.707$</td></tr><tr><td style=text-align:left>$P2 (1,0)$</td><td style=text-align:left>$\sqrt{(1-0)^2+(0-0.5)^2}=1.118$</td><td style=text-align:left>$\sqrt{(1-0.5)^2+(0-0.5)^2}=$ <strong>$0.707$</strong></td></tr><tr><td style=text-align:left>$P3 (0, 1)$</td><td style=text-align:left>$\sqrt{(0-0)^2+(1-0.5)^2}=$ <strong>$0.5$</strong></td><td style=text-align:left>$\sqrt{(0-0.5)^2+(1-0.5)^2}=0.707$</td></tr><tr><td style=text-align:left>$P4 (1,1)$</td><td style=text-align:left>$\sqrt{(1-0)^2+(1-0.5)^2}=1.118$</td><td style=text-align:left>$\sqrt{(1-0.5)^2+(1-0.5)^2}=$ <strong>$0.707$</strong></td></tr></tbody></table><p>So P1 and P3 will be assigned to centroid $m1 (0, 0.5)$ while P2 and P4 will be assigned to centroid $m2 (0.5,0.5)$</p><p><em>Update step:</em></p><p>$$
\begin{aligned}
m_1 &= \frac{1}{2} [(0,0)+(0,1)]=(0,0.5) \newline
m_2 &= \frac{1}{2} [(1,0)+(1,1)]=(1,0.5)
\end{aligned}
$$</p><p>we can visualize the centroids ater the first iteration.</p><p><img src=/images/k-means-clustering/example-iteration2.png alt="k-means example iteration 2"></p><p><strong>Iteration 2:</strong></p><p><em>Assignment step:</em></p><table><thead><tr><th style=text-align:left>Distance to Centroid</th><th style=text-align:left>$m1 (0, 0.5)$</th><th style=text-align:left>$m2 (0.5, 0.5)$</th></tr></thead><tbody><tr><td style=text-align:left>$P1 (0, 0)$</td><td style=text-align:left>$\sqrt{(0-0)^2+(0-0.5)^2}=$ <strong>$0.5$</strong></td><td style=text-align:left>$\sqrt{(0-1.0)^2+(0-0.5)^2}=1.118$</td></tr><tr><td style=text-align:left>$P2 (1,0)$</td><td style=text-align:left>$\sqrt{(1-0)^2+(0-0.5)^2}=1.118$</td><td style=text-align:left>$\sqrt{(1-1.0)^2+(0-0.5)^2}=$ <strong>$0.5$</strong></td></tr><tr><td style=text-align:left>$P3 (0, 1)$</td><td style=text-align:left>$\sqrt{(0-0)^2+(1-0.5)^2}=$ <strong>$0.5$</strong></td><td style=text-align:left>$\sqrt{(0-1.0)^2+(1-0.5)^2}=1.118$</td></tr><tr><td style=text-align:left>$P4 (1,1)$</td><td style=text-align:left>$\sqrt{(1-0)^2+(1-0.5)^2}=1.118$</td><td style=text-align:left>$\sqrt{(1-1.0)^2+(1-0.5)^2 }=$ <strong>$0.5$</strong></td></tr></tbody></table><p>And no new change in assignment will be done and the algorithm terminates.</p><h3 id=python-implementation>Python Implementation</h3><p>We can easily implement the above two steps in a couple of lines of python.</p><div class=highlight><pre tabindex=0 style=color:#f8f8f2;background-color:#272822;-moz-tab-size:4;-o-tab-size:4;tab-size:4><code class=language-python data-lang=python><span style=display:flex><span><span style=color:#66d9ef>def</span> <span style=color:#a6e22e>kMeans</span>(X, k):
</span></span><span style=display:flex><span>    m <span style=color:#f92672>=</span> np<span style=color:#f92672>.</span>random<span style=color:#f92672>.</span>permutation(X)[:k]
</span></span><span style=display:flex><span>    changes <span style=color:#f92672>=</span> <span style=color:#66d9ef>True</span>
</span></span><span style=display:flex><span>    clusters <span style=color:#f92672>=</span> np<span style=color:#f92672>.</span>zeros(X<span style=color:#f92672>.</span>shape[<span style=color:#ae81ff>0</span>])
</span></span><span style=display:flex><span>    <span style=color:#66d9ef>while</span> changes:
</span></span><span style=display:flex><span>        changes <span style=color:#f92672>=</span> <span style=color:#66d9ef>False</span>
</span></span><span style=display:flex><span>        <span style=color:#66d9ef>for</span> i <span style=color:#f92672>in</span> range(len(X)):
</span></span><span style=display:flex><span>            idx <span style=color:#f92672>=</span> np<span style=color:#f92672>.</span>argmin(dist(X[i,:], m))
</span></span><span style=display:flex><span>            <span style=color:#66d9ef>if</span> idx <span style=color:#f92672>!=</span> clusters[i]:
</span></span><span style=display:flex><span>                changes <span style=color:#f92672>=</span> <span style=color:#66d9ef>True</span>
</span></span><span style=display:flex><span>            clusters[i] <span style=color:#f92672>=</span> idx
</span></span><span style=display:flex><span>        
</span></span><span style=display:flex><span>        <span style=color:#66d9ef>for</span> j <span style=color:#f92672>in</span> range(len(m)):
</span></span><span style=display:flex><span>            m[j,:] <span style=color:#f92672>=</span> np<span style=color:#f92672>.</span>sum(X[clusters <span style=color:#f92672>==</span> j,:],axis<span style=color:#f92672>=</span><span style=color:#ae81ff>0</span>)<span style=color:#f92672>/</span>np<span style=color:#f92672>.</span>sum(clusters <span style=color:#f92672>==</span> j)   
</span></span><span style=display:flex><span>            
</span></span><span style=display:flex><span>    <span style=color:#66d9ef>return</span> m, clusters
</span></span></code></pre></div><p>And here is the definition of function <code>dist</code></p><div class=highlight><pre tabindex=0 style=color:#f8f8f2;background-color:#272822;-moz-tab-size:4;-o-tab-size:4;tab-size:4><code class=language-python data-lang=python><span style=display:flex><span><span style=color:#66d9ef>def</span> <span style=color:#a6e22e>dist</span>(x, y):
</span></span><span style=display:flex><span>    <span style=color:#66d9ef>return</span> np<span style=color:#f92672>.</span>sqrt(np<span style=color:#f92672>.</span>sum((x <span style=color:#f92672>-</span> y) <span style=color:#f92672>**</span> <span style=color:#ae81ff>2</span>, axis<span style=color:#f92672>=</span><span style=color:#ae81ff>1</span>))
</span></span></code></pre></div><h3 id=picking-k>Picking K</h3><p>At this point you might be asking the questions how to pick the correct number of clusters because the choice of k can generate completely different result. A good technique is to use the <a href=https://en.wikipedia.org/wiki/Elbow_method_(clustering)>elbow method</a> .</p><p>The elbow method is one of the most popular methods to determine the optimal number of clusters. We plot for different value of K on the x-axis, the distortion on the y-axis. We define distortion as the sum of distance between the data point and the centroid.
Distortion = $\sum \lvert \lvert x_i-m_c \rvert \rvert $. At certain value of K we will notice shift in the graph trend which determines the optimal value of K.</p><p>Let’s start by some arbitrary dataset consisting of three clusters as shown in the next figure, of course in real life we do not know in advance the actual number of clusters but we will use this example to see how the elbow method will do relative to what we know to be true.</p><div class=highlight><pre tabindex=0 style=color:#f8f8f2;background-color:#272822;-moz-tab-size:4;-o-tab-size:4;tab-size:4><code class=language-python data-lang=python><span style=display:flex><span><span style=color:#f92672>from</span> sklearn.datasets <span style=color:#f92672>import</span> make_blobs
</span></span><span style=display:flex><span>x, y <span style=color:#f92672>=</span> make_blobs(n_samples<span style=color:#f92672>=</span><span style=color:#ae81ff>20</span>, centers<span style=color:#f92672>=</span><span style=color:#ae81ff>3</span>, n_features<span style=color:#f92672>=</span><span style=color:#ae81ff>2</span>)
</span></span><span style=display:flex><span>plt<span style=color:#f92672>.</span>plot()
</span></span><span style=display:flex><span>plt<span style=color:#f92672>.</span>title(<span style=color:#e6db74>&#39;Dataset&#39;</span>)
</span></span><span style=display:flex><span>plt<span style=color:#f92672>.</span>scatter(x[:,<span style=color:#ae81ff>0</span>], x[:,<span style=color:#ae81ff>1</span>])
</span></span><span style=display:flex><span>plt<span style=color:#f92672>.</span>show()
</span></span></code></pre></div><p><img src=/images/k-means-clustering/three-clusters.png alt="three clusters example"></p><p>The next step is to apply k-means clustering algorithm for a range of value, calculate the distortion and plot it as a function of K. In this example we will try the k values in the range 1 to 10.</p><div class=highlight><pre tabindex=0 style=color:#f8f8f2;background-color:#272822;-moz-tab-size:4;-o-tab-size:4;tab-size:4><code class=language-python data-lang=python><span style=display:flex><span>distortions <span style=color:#f92672>=</span> []
</span></span><span style=display:flex><span>K <span style=color:#f92672>=</span> range(<span style=color:#ae81ff>1</span>,<span style=color:#ae81ff>10</span>)
</span></span><span style=display:flex><span><span style=color:#66d9ef>for</span> k <span style=color:#f92672>in</span> K:
</span></span><span style=display:flex><span>    m, c <span style=color:#f92672>=</span> kMeans(x, k)
</span></span><span style=display:flex><span>    distortion <span style=color:#f92672>=</span> <span style=color:#ae81ff>0</span>
</span></span><span style=display:flex><span>    <span style=color:#66d9ef>for</span> i <span style=color:#f92672>in</span> range(len(x)):
</span></span><span style=display:flex><span>            distortion <span style=color:#f92672>=</span> distortion <span style=color:#f92672>+</span> np<span style=color:#f92672>.</span>min(dist(x[i,:], m))
</span></span><span style=display:flex><span>    distortions<span style=color:#f92672>.</span>append(distortion<span style=color:#f92672>/</span> x<span style=color:#f92672>.</span>shape[<span style=color:#ae81ff>0</span>])
</span></span></code></pre></div><p>We can then plot the distortions as a function of k. we then notice the elbow shape the curve makes at K=3 indicating the optimal value for the number of clusters.</p><div class=highlight><pre tabindex=0 style=color:#f8f8f2;background-color:#272822;-moz-tab-size:4;-o-tab-size:4;tab-size:4><code class=language-python data-lang=python><span style=display:flex><span>plt<span style=color:#f92672>.</span>plot(K, distortions, <span style=color:#e6db74>&#39;*-&#39;</span>)
</span></span><span style=display:flex><span>plt<span style=color:#f92672>.</span>xlabel(<span style=color:#e6db74>&#39;k&#39;</span>)
</span></span><span style=display:flex><span>plt<span style=color:#f92672>.</span>ylabel(<span style=color:#e6db74>&#39;Distortion&#39;</span>)
</span></span><span style=display:flex><span>plt<span style=color:#f92672>.</span>title(<span style=color:#e6db74>&#39;The Elbow Method&#39;</span>)
</span></span><span style=display:flex><span>plt<span style=color:#f92672>.</span>show()
</span></span></code></pre></div><p><img src=/images/k-means-clustering/elbow-method.png alt="elbow method"></p><h3 id=shortcomings>Shortcomings</h3><p>Because we are using the Euclidean distance when measuring the distance to the centroids, K-means algorithm prefer clusters that have circular shapes even when a better cluster shape is more appropriate. For example, let’s look at dataset of two interleaving circles (<a href=https://scikit-learn.org/stable/modules/generated/sklearn.datasets.make_moons.html>make_moons in sklearn</a>).</p><div class=highlight><pre tabindex=0 style=color:#f8f8f2;background-color:#272822;-moz-tab-size:4;-o-tab-size:4;tab-size:4><code class=language-python data-lang=python><span style=display:flex><span><span style=color:#f92672>from</span> sklearn.datasets <span style=color:#f92672>import</span> make_moons
</span></span><span style=display:flex><span>x, y <span style=color:#f92672>=</span> make_moons(n_samples<span style=color:#f92672>=</span><span style=color:#ae81ff>20</span>)
</span></span><span style=display:flex><span>plt<span style=color:#f92672>.</span>plot()
</span></span><span style=display:flex><span>plt<span style=color:#f92672>.</span>title(<span style=color:#e6db74>&#39;Two interleaving half-circles&#39;</span>)
</span></span><span style=display:flex><span>plt<span style=color:#f92672>.</span>scatter(x[:,<span style=color:#ae81ff>0</span>], x[:,<span style=color:#ae81ff>1</span>],c<span style=color:#f92672>=</span>y)
</span></span><span style=display:flex><span>plt<span style=color:#f92672>.</span>show()
</span></span></code></pre></div><p><img src=/images/k-means-clustering/make_moons.png alt="make moons example"></p><p>However, if we try to apply K-means algorithm using two clustering we will get two different clusters. Although it is a valid clustering it might not be the best way to divide the dataset to make inferences about the dataset.</p><p><img src=/images/k-means-clustering/make_moons_clusterd.png alt="make moons clustered"></p><h3 id=matrix-factorization-perspective>Matrix Factorization Perspective</h3><p>I learnt about this idea from the fantastic book <a href=https://www.amazon.com/Machine-Learning-Refined-Foundations-Applications/dp/1108480721>Machine Learning Refined book</a>. The books is filled with many insights about machine learning algorithms from an optimization point of you and I encourage you to check it out.</p><p>In our implementation, there are a set of points $X$ and the aim is to find K cluster centroids $C$ and assign each point $x$ to a particular cluster $c_k$ so that the assigned centroid is as close to the point as possible. i.e., if $e_k$ is all zeros vector except at entry k, then we want:</p><p>$$
C e_k = c_k \approx x_p \quad \text{for all points} \quad x_p \in X
$$</p><p>The trick is we can rewrite this objective in matrix notation as $CW \approx X$, and lo and behold we arrive at the canonical <a href=https://developers.google.com/machine-learning/recommendation/collaborative/matrix>matrix factorization from</a>. In this from, $W$ is the weight matrix with the special constraint that each column contains a single nonzero entry that determines assignment of points to clusters. The next figure shows this framing for our <a href=#worked-example>worked example</a>.</p><p><img src=/images/k-means-clustering/mf.png alt="k-means as matrix factorization"></p><p>In this framing, K-means can be analyzed as an optimization problem with an objective function to be minimized and we can track and analyze the progress of the algorithm on each iteration.</p><h3 id=conclusion>Conclusion</h3><p>In this post, we looked at the K-means clustering algorithm. We then looked at the algorithm steps and visualized some examples. We also discussed how to determine the optimal number of clusters and when the algorithm might not produce the best results. Finally, we discussed how K-means, a heuristic algorithm, can be viewed from the lens of an optimization framework.</p></div></div></section><section class=section><div class="container has-text-centered"><p></p></div></section><script src=//cdn.jsdelivr.net/gh/highlightjs/cdn-release@10.2.0/build/highlight.min.js></script><script>hljs.initHighlightingOnLoad()</script></body>